{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHXz3a0AVzY1Ho2On4Hbc+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "176998f5585f4e3dabb9934e5c3cc467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0dea3144f88c421a9c6554d66a0953a1",
              "IPY_MODEL_3652b453dbeb4e7ab1fd27389e1af446",
              "IPY_MODEL_0e6799808c124605893f8c12a9e3a424"
            ],
            "layout": "IPY_MODEL_3251b0f88e82431ab3dce9f0543996cf"
          }
        },
        "0dea3144f88c421a9c6554d66a0953a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31c13517fd9b4652bb943889e277cf50",
            "placeholder": "​",
            "style": "IPY_MODEL_eaa40cc3e82d4627bc45e88b220bde17",
            "value": "Map: 100%"
          }
        },
        "3652b453dbeb4e7ab1fd27389e1af446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_118596f8c78f4ec4a6d28dd46c292a23",
            "max": 408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec815e931bea4c5b8fd1d294dc8be71a",
            "value": 408
          }
        },
        "0e6799808c124605893f8c12a9e3a424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8d0c72b273146e19f4e5997d7251f10",
            "placeholder": "​",
            "style": "IPY_MODEL_f0e730b89b1649e099194653245c6d6b",
            "value": " 408/408 [00:00&lt;00:00, 969.97 examples/s]"
          }
        },
        "3251b0f88e82431ab3dce9f0543996cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c13517fd9b4652bb943889e277cf50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaa40cc3e82d4627bc45e88b220bde17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "118596f8c78f4ec4a6d28dd46c292a23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec815e931bea4c5b8fd1d294dc8be71a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8d0c72b273146e19f4e5997d7251f10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0e730b89b1649e099194653245c6d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1b81effdcfa420ca1424e9dba361c9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e1dd2bc927140149d6686782a20203e",
              "IPY_MODEL_7a1a7f13c6c041678382df21b8b62151",
              "IPY_MODEL_c722689ce609435c8084581d12b89869"
            ],
            "layout": "IPY_MODEL_98a70125b8504e499ac312543ef800ae"
          }
        },
        "5e1dd2bc927140149d6686782a20203e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84c875352f1b402d9093ef429ccd8ca5",
            "placeholder": "​",
            "style": "IPY_MODEL_66af1a81669740888ee9daaee1400398",
            "value": "Map: 100%"
          }
        },
        "7a1a7f13c6c041678382df21b8b62151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59cf09b1530e464c8b9bdb06240f18e3",
            "max": 408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5edbcca5498a4d07a98711b2702ef0be",
            "value": 408
          }
        },
        "c722689ce609435c8084581d12b89869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf64282e8f704c7fb995df828f9f5361",
            "placeholder": "​",
            "style": "IPY_MODEL_b50527a38320488083dad41cd5aff404",
            "value": " 408/408 [00:00&lt;00:00, 1775.80 examples/s]"
          }
        },
        "98a70125b8504e499ac312543ef800ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84c875352f1b402d9093ef429ccd8ca5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66af1a81669740888ee9daaee1400398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59cf09b1530e464c8b9bdb06240f18e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5edbcca5498a4d07a98711b2702ef0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf64282e8f704c7fb995df828f9f5361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b50527a38320488083dad41cd5aff404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karsarobert/NLP2025/blob/main/04/NLP2024_04_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modellek finomhangolása"
      ],
      "metadata": {
        "id": "oDR-tY39WqZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJzTT9BcXalb",
        "outputId": "21f3f3f9-68e2-47e3-cc46-ca8cf953b7d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (4.25.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PsV9he8U9mk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2e4e5ac-5160-4524-f295-75e0ca87037d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# PyTorch és Transformers könyvtárak importálása\n",
        "import torch\n",
        "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Előre definiált modell nevének kiválasztása\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "\n",
        "# Tokenizáló létrehozása az előre definiált modell alapján\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Modell inicializálása a prediktáláshoz és finomhangoláshoz az előre definiált modell alapján\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "# Input szekvenciák előkészítése a tokenizáló segítségével\n",
        "sequences = [\n",
        "    \"I've been waiting for a NLP course my whole life.\",\n",
        "    \"This course is amazing!\",\n",
        "]\n",
        "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Címkék hozzáadása a batch-hez (0 vagy 1 az adott inputra)\n",
        "batch[\"labels\"] = torch.tensor([1, 1])\n",
        "\n",
        "# Optimizer inicializálása az AdamW segítségével a modell paramétereivel\n",
        "optimizer = AdamW(model.parameters())\n",
        "\n",
        "# A veszteség kiszámítása és visszaterjesztése a hálózaton\n",
        "loss = model(**batch).loss\n",
        "loss.backward()\n",
        "\n",
        "# A gradiensek alapján az optimalizáló lépés végrehajtása\n",
        "optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Természetesen a modellnek mindössze két mondaton való edzése nem fog túl jó eredményeket hozni. Ahhoz, hogy jobb eredményeket érj el, nagyobb adathalmazt kell összeállítanod.**\n",
        "\n",
        "**Ebben a részben példaként az MRPC (Microsoft Research Paraphrase Corpus) adathalmazt fogjuk használni, amelyet William B. Dolan és Chris Brockett egy tanulmányukban mutattak be. Az adathalmaz 5801 mondatpárt tartalmaz, egy jelzéssel, ami azt mutatja, hogy parafrázisok-e vagy sem (azaz, hogy mindkét mondat ugyanazt jelenti-e).**\n"
      ],
      "metadata": {
        "id": "ZXymX0sfWxSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A Hub nem csak modelleket tartalmaz; rengeteg adathalmazt is megtalálsz itt, sok különböző nyelven. Böngészhetsz az adathalmazok között itt: [https://huggingface.co/datasets](https://huggingface.co/datasets), és azt javasoljuk, hogy próbálj meg betölteni és feldolgozni egy új adathalmazt, miután átnézted ezt a részt (lásd az általános dokumentációt itt: [https://huggingface.co/docs](https://huggingface.co/docs)).  De most egyelőre koncentráljunk az MRPC adathalmazra! Ez a 10 adathalmaz egyike, amely a GLUE benchmarkot alkotja. Ez egy olyan tudományos mérce, amely arra szolgál, hogy megmérje az ML-modellek teljesítményét 10 különböző szövegosztályozási feladatban.**\n",
        "\n",
        "**A 🤗 Datasets könyvtár egy nagyon egyszerű parancsot biztosít a Hubon lévő adathalmazok letöltésére és gyorsítótárazására. Az MRPC adathalmazt az alábbiak szerint tölthetjük le:**\n"
      ],
      "metadata": {
        "id": "VODho35nXE8A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXynwHxVU9ml",
        "outputId": "1be1c732-8d57-4b5c-e425-e232f9b9052a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 3668\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 408\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 1725\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# A `datasets` könyvtárból importáljuk a `load_dataset` függvényt, amely lehetővé teszi adatkészletek betöltését.\n",
        "from datasets import load_dataset\n",
        "\n",
        "# \"glue\" adatkészletből a \"mrpc\" alcsoport betöltése.\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "\n",
        "# A betöltött adatkészlet visszaadása.\n",
        "raw_datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ahogy láthatod, egy DatasetDict objektumot kapunk, ami a tréninghalmazt, a validációs halmazt és a teszthalmazt tartalmazza. Ezek mindegyike több oszlopot foglal magában (sentence1, sentence2, label és idx), valamint változó számú sort, melyek az egyes halmazokban található elemek számai (tehát 3668 pár mondat van a tréninghalmazban, 408 a validációs halmazban és 1725 a teszthalmazban).**\n",
        "\n",
        "**A letöltött és tárolt adathalmazt alapértelmezés szerint a ~/.cache/huggingface/datasets helyen találhatjuk meg.  A tároló mappádat a HF_HOME környezeti változó beállításával testre szabhatod.**\n",
        "\n",
        "**A raw_dataset objektumunkban lévő mondatpárokat indexeléssel érhetjük el, akárcsak egy szótár (dictionary) esetében:**\n"
      ],
      "metadata": {
        "id": "6m5EnhMVXrmw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-BqdzOFU9mm",
        "outputId": "5afc5e5f-801e-4721-f8a8-a58a547ad2e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
              " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
              " 'label': 1,\n",
              " 'idx': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "raw_train_dataset = raw_datasets[\"train\"]\n",
        "raw_train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Láthatjuk, hogy a címkék már egész számok, így itt nem kell előfeldolgozást végeznünk. Ahhoz, hogy megtudjuk, melyik egész szám melyik címkének felel meg, megvizsgálhatjuk a raw_train_datasetünk jellemzőit. Ebből megtudjuk, hogy az egyes oszlopok milyen típusúak:"
      ],
      "metadata": {
        "id": "zwzLyZPqX-nC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE_blOBPU9mn",
        "outputId": "0a4fda90-048b-401b-a9d6-40ea06213575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence1': Value(dtype='string', id=None),\n",
              " 'sentence2': Value(dtype='string', id=None),\n",
              " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
              " 'idx': Value(dtype='int32', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "raw_train_dataset.features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A színfalak mögött a címke ClassLabel típusú, és az egész számok címkenévhez való hozzárendelése a names mappában van tárolva. A 0 megfelel a not_equivalentnek, az 1 pedig az equivalentnek."
      ],
      "metadata": {
        "id": "rWnIWMDvYTuK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz2wmHmeU9mn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
        "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Azonban nem adhatunk át két szekvenciát a modellnek, és nem kaphatunk előrejelzést arról, hogy a két mondat parafrázis-e vagy sem. A két szekvenciát párként kell kezelnünk, és megfelelő előfeldolgozást kell alkalmaznunk. Szerencsére a tokenizáló is képes egy szekvenciapárt fogadni, és úgy előkészíteni, ahogy a BERT-modellünk elvárja:"
      ],
      "metadata": {
        "id": "iH4tUT1MYpKW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw6BcSVtU9mn",
        "outputId": "78c11b6b-0c2d-49cd-8254-6297affb8c78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8eoomTLU9mn",
        "outputId": "679ff779-2184-4a9a-b4e5-8d39f534a576",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'sentence',\n",
              " '.',\n",
              " '[SEP]',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'second',\n",
              " 'one',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Láthatjuk tehát, hogy a modell a bemeneteket [CLS] mondat1 [SEP] mondat2 [SEP] formában várja el, ha két mondat van. Ezt a token_type_ids értékkel összehangolva megkapjuk:"
      ],
      "metadata": {
        "id": "mNCWA-fBZGsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ahogy láthatod, a bemenet azon részei, amelyek a [CLS] sentence1 [SEP] -nek felelnek meg, mind 0-s típus azonosítóval (token type ID) rendelkeznek, míg a többi, a sentence2 [SEP] -nek megfelelő részek mind 1-es típussal vannak ellátva.**\n",
        "\n",
        "**Fontos megjegyezni, hogy ha más ellenőrzőpontot (checkpoin-ot) választasz, nem feltétlenül kapsz token_type_ids-t a tokenizált bemenetekben (például, ha egy DistilBERT modellt használsz, azok nem kerülnek visszaadásra). Csak akkor lesznek visszaadva, ha a modell tudni fogja, mit kezdjen velük, mivel az előképzés során már találkozott velük.**\n",
        "\n",
        "**A BERT itt token típusa ID-kkel kerül előképzésre és a már tárgyalt maszkolt nyelvi modellezési cél mellett van egy kiegészítő feladata, az úgynevezett \"következő mondat előrejelzése\" (next sentence prediction). Ennek a feladatnak az a célja, hogy modellezze a mondatpárok közötti kapcsolatot.**\n",
        "\n",
        "**A következő mondat előrejelzésekor a modell kap egy pár mondatot (véletlenszerűen maszkolt tokenekkel) és meg kell jósolnia, hogy a második mondat követi-e az elsőt. A feladat nehezítése érdekében az esetek felében a mondatok egymás után következnek az eredeti dokumentumban, amelyből kinyerésre kerültek, másik felében pedig két teljesen különböző dokumentumból származnak.**\n",
        "\n",
        "**Általánosságban nem kell aggódnod, hogy vannak-e token_type_ids-k a tokenizált bemenetedben: amíg ugyanazt az ellenőrzőpontot használod a tokenizer és a modell esetében, minden rendben lesz, mivel a tokenizer tudja, mit kell biztosítani a modelljének.**\n",
        "\n",
        "**Mivel láttuk, hogyan tud a tokenizerünk egy pár mondattal foglalkozni, használhatjuk a teljes adathalmaz tokenizálására: az előző gyakorlathoz hasonlóan a tokenizernek átadhatunk egy listát a mondatpárokból, úgy, hogy megadjuk az első mondatok, majd a második mondatok listáját. Ez kompatibilis a  kitöltési (padding) és csonkítási (truncation) beállításokkal is. Tehát az egyik módja a tréning adathalmaz előfeldolgozásának:**\n"
      ],
      "metadata": {
        "id": "bxnhoHEVZYpR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdzAYHTnU9mo"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenizer(\n",
        "    raw_datasets[\"train\"][\"sentence1\"],\n",
        "    raw_datasets[\"train\"][\"sentence2\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ez a megoldás jól működik, de van egy hátránya:  egy szótárat ad vissza (a kulcsainkkal: input_ids, attention_mask és token_type_ids; és azokkal az értékekkel, amik listák listái). Valamint csak akkor fog rendesen működni, ha elég RAM-mal rendelkezel ahhoz, hogy a teljes adathalmazon elvégezd a tokenizációt (míg a 🤗 Datasets könyvtárból származó adathalmazok Apache Arrow fájlok, a lemezen tárolva, így csak a memóriába töltöd azokat a mintákat, amelyeket lekérdezel).**\n",
        "\n",
        "**Ahhoz, hogy az adatot adathalmaz formában tartsuk, a Dataset.map() metódust fogjuk használni. Ez lehetőséget ad extra rugalmasságra is, ha nem csak a tokenizációnál van szükségünk további előfeldolgozásra. A map() metódus úgy működik, hogy minden elemre alkalmaz egy adott függvényt, szóval alkossunk egy olyan függvényt, ami tokenizálja a bemeneteinket:**\n"
      ],
      "metadata": {
        "id": "nW69IvrgaQBv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbz2nInWU9mo"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ez a függvény egy szótárat vesz fel (például az adathalmazunk elemeit) és egy új szótárt ad vissza input_ids, attention_mask és token_type_ids kulcsokkal. Fontos megjegyezni, hogy akkor is működik, ha az 'example' szótár több mintát tartalmaz (mindegyik kulcs egy mondatlista), hiszen a tokenizer képes mondatpárok listáján dolgozni, ahogyan azt korábban láttuk. Ez lehetővé teszi számunkra a batched=True opció használatát a map() hívásunkban, ami jelentős mértékben felgyorsítja a tokenizációt. A tokenizer mögött a 🤗Tokenizers könyvtárból való, Rust nyelven írt tokenizer működik. Ez a tokenizer nagyon gyors lehet, de csak akkor, ha nagy mennyiségű bemenetet kap egyszerre.**\n",
        "\n",
        "**Megjegyzés: szándékosan most nem használtuk a padding  argumentumot a tokenizációs függvényünkben. Ennek az az oka, hogy nem hatékony minden mintát a maximális hosszúságra kitölteni (padding): sokkal jobb a mintákat akkor kitölteni, amikor egy köteget (batch) építünk, hiszen így arra elegendő csak a maximális hosszúságra kitölteni, és nem a teljes adathalmazéra. Ez rengeteg időt és számítási kapacitást spórolhat meg, amikor a bemenetek nagymértékben változnak hosszúságban!**\n",
        "\n",
        "Így alkalmazhatod a tokenizációs függvényt minden adathalmazodra egyszerre. A  batched=True-t használjuk a map hívásban, így a függvény a dataset-ünk több elemére lesz egyszerre alkalmazva, így  nem minden elemre külön-külön. Ez gyorsabb előfeldolgozást tesz lehetővé.\n",
        "\n",
        "Amikor a map() függvényt használjuk, általában egy függvényt adunk meg paraméterként, amelyet szeretnénk alkalmazni az iterálható objektum elemeire. Ezt követően azon iterálható objektumokat adjuk meg, amelyeken a függvényt alkalmazni szeretnénk.\n"
      ],
      "metadata": {
        "id": "1Ni1aAgXb4S_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UsgBAV_U9mp",
        "outputId": "71543edc-27c4-4d81-85dd-d05877fa45b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "176998f5585f4e3dabb9934e5c3cc467",
            "0dea3144f88c421a9c6554d66a0953a1",
            "3652b453dbeb4e7ab1fd27389e1af446",
            "0e6799808c124605893f8c12a9e3a424",
            "3251b0f88e82431ab3dce9f0543996cf",
            "31c13517fd9b4652bb943889e277cf50",
            "eaa40cc3e82d4627bc45e88b220bde17",
            "118596f8c78f4ec4a6d28dd46c292a23",
            "ec815e931bea4c5b8fd1d294dc8be71a",
            "a8d0c72b273146e19f4e5997d7251f10",
            "f0e730b89b1649e099194653245c6d6b"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "176998f5585f4e3dabb9934e5c3cc467"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 3668\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 408\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 1725\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A map() használatakor akár többprocesszos (multiprocessing) megoldást is használhatsz az előfeldolgozási funkciódhoz a num_proc argumentum megadásával. Most nem tettük ezt, mert a 🤗 Tokenizers könyvtár már több szálat használ a gyorsabb tokenizáció érdekében, ha viszont nem egy ilyen gyors tokenizerrel dolgozol, akkor ez felgyorsíthatja az előfeldolgozási folyamatot.**\n",
        "\n",
        "A tokenize_function függvényünk egy szótárt ad eredményül, amiben szerepelnek az input_ids, attention_mask és token_type_ids kulcsok. Ennek köszönhetően mindhárom mezőt minden adathalmazunk minden részhalmazához hozzáadjuk. Fontos, hogy létező mezőket is tudnánk módosítani, ha az előfeldolgozó függvényünk egy új értéket adna eredményül arra a kulcsra, amely már létezik abban az adathalmazban, amire a map()-ot alkalmazzuk.\n",
        "\n",
        "**Az utolsó teendőnk annyi, hogy minden példányt a leghosszabb elem hosszára töltsünk ki, amikor ezeket az elemeket kötegekbe (batch-ekbe) gyűjtjük. Ezt a technikát dinamikus kitöltésnek (dynamic padding) nevezzük.**\n"
      ],
      "metadata": {
        "id": "4tGkWU9vccfl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7yTRwMpU9mp"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Az elemek köteggé (batch) alakításáért a collate függvény a felelős. Ez egy olyan paraméter, amit átadhatsz a DataLoader létrehozásakor. Az alapértelmezett működés egyszerűen PyTorch tenzorokká alakítja a mintákat és összefűzi őket (rekurzívan, ha az elemeid listák, tuple-ök vagy szótárak). Ez nem lesz megfelelő a mi esetünkben, mivel a bemeneteink különböző hosszúságúak lesznek. Szándékosan elhalasztottuk a kitöltést (padding), hogy csak akkor végezzük, ha valóban szükséges az adott kötegen, ezzel elkerüljük a túl hosszú, rengeteg paddinget tartalmazó bemeneteket. Ez felgyorsítja a tréninget, de a TPU használatakor problémákat okozhat, mivel a TPU-k a fix formát kedvelik, még akkor is, ha ez extra kitöltést igényel.**\n",
        "\n",
        "**Gyakorlatban ehhez definiálnunk kell egy collate függvényt, ami a megfelelő mennyiségű paddinget alkalmazza az egy kötegbe kerülő elemekre. Szerencsére a 🤗 Transformers könyvtár biztosít ilyen funkciót, ez a DataCollatorWithPadding. Létrehozásakor felvesz egy tokenezőt (hogy tudja melyik padding token-t használja, valamint, hogy a modell a bemenetek melyik oldalán várja a paddinget) és minden mást elintéz, amire szükségünk van:**\n"
      ],
      "metadata": {
        "id": "eFuIgMPSdN5W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr6Am-4AU9mp"
      },
      "outputs": [],
      "source": [
        "# Az első 8 minta kiválasztása a \"train\" részből\n",
        "samples = tokenized_datasets[\"train\"][:8]\n",
        "\n",
        "# Az \"idx\", \"sentence1\" és \"sentence2\" kulcsok eltávolítása a mintákból\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "\n",
        "# Az input tokenek hosszának listába gyűjtése az összes maradék mintára\n",
        "lengths = [len(x) for x in samples[\"input_ids\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nem meglepő, hogy különböző hosszúságú mintákat kapunk, 32-től 67-ig. A dinamikus kitöltés azt jelenti, hogy a tételben lévő mintákat mind 67 hosszúságúra kell kitölteni, ami a tételen belüli maximális hossz. Dinamikus kitöltés nélkül az összes mintát a teljes adathalmaz maximális hosszára kellene kitölteni, vagy a modell által elfogadható maximális hosszra. Ellenőrizzük le, hogy a data_collatorunk megfelelően dinamikusan kitölti a köteget:"
      ],
      "metadata": {
        "id": "QQGm8Zadducl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeP4LtsxU9mp",
        "outputId": "8893db2b-da02-46fc-9de6-dfcaad44e6fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': torch.Size([8, 67]),\n",
              " 'token_type_ids': torch.Size([8, 67]),\n",
              " 'attention_mask': torch.Size([8, 67]),\n",
              " 'labels': torch.Size([8])}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Adatösszeállító (data collator) függvény alkalmazása a mintákra\n",
        "batch = data_collator(samples)\n",
        "\n",
        "# Az összeállított batch elemeinek alakjának lekérése és tárolása egy szótárban\n",
        "{k: v.shape for k, v in batch.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A Transformers egy Trainer osztállyal segít előkészített modelljeid finomhangolásában a saját adathalmazodon. A legnehezebb rész vélhetően annak a környezetnek az előkészítése lesz, ahol a Trainer.train() lefut, mivel CPU-n rendkívül lassan fog menni. Ha nincs GPU-d, ingyenes GPU-hoz vagy TPU hozzáférést szerezhetsz a Google Colab.**\n",
        "\n"
      ],
      "metadata": {
        "id": "2SqSGW-_gIat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Képzés\n",
        "Az első lépés, mielőtt definiálhatnánk a trénerünket, egy TrainingArguments osztály definiálása, amely tartalmazza az összes hiperparamétert, amelyet a tréner a képzéshez és a kiértékeléshez használni fog. Az egyetlen argumentum, amit meg kell adnunk, az egy könyvtár, ahová a betanított modellt elmentjük, valamint az ellenőrző pontok az út mentén. Az összes többihez meghagyhatja az alapértelmezetteket, amelyeknek elég jól kell működniük egy alapvető finomhangoláshoz."
      ],
      "metadata": {
        "id": "uIXtA5jAgiza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKIx0YBsg-Bp",
        "outputId": "c397e7da-b7d6-4250-e3a7-7d196b316d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.28.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uco_bc8iw4Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQIWSUQYwr4G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d1b81effdcfa420ca1424e9dba361c9b",
            "5e1dd2bc927140149d6686782a20203e",
            "7a1a7f13c6c041678382df21b8b62151",
            "c722689ce609435c8084581d12b89869",
            "98a70125b8504e499ac312543ef800ae",
            "84c875352f1b402d9093ef429ccd8ca5",
            "66af1a81669740888ee9daaee1400398",
            "59cf09b1530e464c8b9bdb06240f18e3",
            "5edbcca5498a4d07a98711b2702ef0be",
            "bf64282e8f704c7fb995df828f9f5361",
            "b50527a38320488083dad41cd5aff404"
          ]
        },
        "outputId": "66ab26ff-803b-4e68-a06f-bf9f1861fac2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1b81effdcfa420ca1424e9dba361c9b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yU929onceIKE"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"test-trainer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3kRtUYYeIKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e1a8b7-9f3c-4ec8-eabd-6a610753428f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Látni fogod, hogy az előző órával ellentétben egy figyelmeztetést kapsz az előretrénelt modell példányosítása után.  Azért van ez, mert a BERT-et nem mondatpárok osztályozására képezték elő, tehát az előtrénelt modell fejrésze (head) el lett dobva, helyette egy új, szekvenciák osztályozására alkalmas fej került beillesztésre. A figyelmeztetés jelzi, hogy egyes súlyok (weights) nem kerültek felhasználásra (az eltávolított előtrénelési fejnek megfelelőek), illetve mások véletlenszerűen lettek inicializálva (az új fejhez tartozók). Végül arra bíztat, hogy tanítsuk be a modellt, ami pontosan a következő lépésünk lesz.**  \n",
        "\n",
        "Miután megvan a modellünk, definiálhatjuk a Trainer-t a következők átadásával: a modell, a training_args, a tréning és validációs adathalmazok, az adat betöltő-nk (data_collator) és a tokenizer-ünk.\n"
      ],
      "metadata": {
        "id": "WA4gbMLVhsdh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6dka727eIKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba8e9668-3f7e-4ce6-dcb9-0613116ac1f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-36fd205b194c>:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fontos megjegyezni, amikor a tokenizer-t átadod, mint ahogy mi is tettük itt, a Trainer által használt alapértelmezett adat betöltő (data_collator) egy DataCollatorWithPadding lesz, mint amilyet az előbb definiáltunk, tehát kihagyhatod a  data_collator=data_collator sort ebből a hívásból.**\n",
        "\n",
        "A modell finomhangolásához adathalmazunkon egyszerűen hívjuk a Trainer train() metódusát:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-WRpP2XrjRp6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmKmVm39eIKF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "08dc2c07-90d7-488b-fed5-44ddffa9a062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarsarobert\u001b[0m (\u001b[33mkarsar\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250225_204415-jtnbe6im</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/karsar/huggingface/runs/jtnbe6im' target=\"_blank\">test-trainer</a></strong> to <a href='https://wandb.ai/karsar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/karsar/huggingface' target=\"_blank\">https://wandb.ai/karsar/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/karsar/huggingface/runs/jtnbe6im' target=\"_blank\">https://wandb.ai/karsar/huggingface/runs/jtnbe6im</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1377/1377 2:20:51, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.572400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.438000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1377, training_loss=0.46233711990655635, metrics={'train_runtime': 8469.9209, 'train_samples_per_second': 1.299, 'train_steps_per_second': 0.163, 'total_flos': 405114969714960.0, 'train_loss': 0.46233711990655635, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ez elindítja a finomhangolást (aminek GPU-n pár percet kell igénybe vennie) és a tréning veszteségét 500-anként kiírja. Azonban ez nem fogja megmondani, hogy a modellünk milyen jól (vagy rosszul) teljesít. Ennek oka:\n",
        "\n",
        "*  Nem állítottuk be a Trainer-t, hogy kiértékelést végezzen tréning közben. Ehhez az evaluation_strategy-t vagy \"steps\"-re (kiértékelés valahány eval_steps lépésenként) vagy \"epoch\"-ra kell állítani (kiértékelés minden korszak végén).\n",
        "* Nem adtunk át a Trainer-nek egy compute_metrics() függvényt, hogy mérőszámokat számítson a kiértékelés során (különben a kiértékelés csak a veszteséget írta volna ki, ami önmagában nem túl informatív).\n",
        "\n",
        "**Kiértékelés (Evaluation)**\n",
        "\n",
        "Nézzük meg, hogyan építhetünk egy hasznos compute_metrics() függvényt és alkalmazhatjuk következő tréning során. A függvénynek fel kell vennie egy EvalPrediction objektumot (aminek egy predictions és egy label_ids mezője van) és egy szótárat kell adnia eredményül, amely stringeket lebegőpontos értékekre képez le (a stringek a mérőszámok nevei, a floa-ok pedig az értékeik). A modell jóslataihoz (prediction) a Trainer.predict() parancsot használhatjuk:"
      ],
      "metadata": {
        "id": "6QgjslKGjWjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
        "```\n",
        "\n",
        "```\n",
        "(408, 2) (408,)\n",
        "```\n",
        "\n",
        "A `predict()` metódus kimenete egy másik, elnevezett tuple három mezővel: `predictions`, `label_ids` és `metrics`. A `metrics` mező csak az átadott adathalmazon bekövetkezett veszteséget, valamint néhány időzítésre vonatkozó metrikát (a predikcióhoz szükséges teljes időt és az átlagos időt) fogja tartalmazni. Miután elkészítjük a `compute_metrics()` függvényünket, és átadjuk a `Trainer`-nek, ez a mező a `compute_metrics()` által visszaadott metrikákat is tartalmazni fogja.\n",
        "\n",
        "Amint láthatod, a `predictions` egy kétdimenziós tömb, amelynek alakja 408 x 2 (a 408 az általunk használt adathalmazban szereplő elemek száma). Ezek a logitek az általunk a `predict()` függvénynek átadott adathalmaz minden elemére (ahogy az előző fejezetben láttad, minden Transformer modell logiteket ad vissza). Ahhoz, hogy olyan predikciókká alakítsuk át őket, amelyeket a címkéinkkel össze tudunk hasonlítani, a második tengelyen a maximális értékkel rendelkező indexet kell figyelembe vennünk:\n"
      ],
      "metadata": {
        "id": "FOys22aPugIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "print(predictions.predictions.shape, predictions.label_ids.shape)"
      ],
      "metadata": {
        "id": "rrMFVzu3aeDX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e35f6e2a-f696-45c3-bdba-3f9a46321efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(408, 2) (408,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "preds = np.argmax(predictions.predictions, axis=-1)"
      ],
      "metadata": {
        "id": "DECN6rRau2Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most már össze tudjuk hasonlítani ezeket a predeket a címkékkel. A compute_metric() függvényünk felépítéséhez a 🤗 Evaluate könyvtár metrikáira támaszkodunk. Az MRPC-adatkészlethez kapcsolódó metrikákat ugyanolyan egyszerűen betölthetjük, mint ahogy az adatkészletet betöltöttük, ezúttal az evaluate.load() függvénnyel. A visszaküldött objektum rendelkezik egy compute() metódussal, amellyel elvégezhetjük a metrika kiszámítását:"
      ],
      "metadata": {
        "id": "p1gwX92hvHW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "metric.compute(predictions=preds, references=predictions.label_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqOuHJ6tvLIH",
        "outputId": "b08671b6-5ce5-4571-838f-2f8d2974c70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8406862745098039, 'f1': 0.8896434634974533}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pontos eredmények változhatnak, mivel a modellfej véletlenszerű inicializálása megváltoztathatja az elért mérőszámokat. Itt láthatjuk, hogy a modellünk 85,78%-os pontosságot ért el a validációs halmazon, és 89,97-es F1-pontszámot. Ez az a két metrika, amelyet a GLUE benchmark MRPC adathalmazon elért eredmények értékeléséhez használtunk. A BERT-dokumentumban található táblázat 88,9 F1-pontszámot jelentett az alapmodellre. Ez a modell uncased volt, míg mi jelenleg a cased modellt használjuk, ami megmagyarázza a jobb eredményt.\n",
        "\n",
        "Mindent összetekerve megkapjuk a compute_metrics() függvényünket:"
      ],
      "metadata": {
        "id": "Pl6kgNbXvMAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "007i5slvvr3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e0dgKTXGvtCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZimfQ-cJvtNi",
        "outputId": "0d5a5934-de10-4255-952b-05deb0a968dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-25-415be425bb02>:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "H0S5z-yAvzWI",
        "outputId": "e6be0565-1971-4a93-80da-684d00f06a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='729' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 729/1377 1:18:50 < 1:10:16, 0.15 it/s, Epoch 1.59/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.394416</td>\n",
              "      <td>0.855392</td>\n",
              "      <td>0.897747</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1377/1377 2:30:36, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.394416</td>\n",
              "      <td>0.855392</td>\n",
              "      <td>0.897747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.540200</td>\n",
              "      <td>0.427499</td>\n",
              "      <td>0.850490</td>\n",
              "      <td>0.896785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.335500</td>\n",
              "      <td>0.620925</td>\n",
              "      <td>0.843137</td>\n",
              "      <td>0.891156</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1377, training_loss=0.37138249792737554, metrics={'train_runtime': 9046.4491, 'train_samples_per_second': 1.216, 'train_steps_per_second': 0.152, 'total_flos': 405114969714960.0, 'train_loss': 0.37138249792737554, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o0jepz5Z2LDy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}