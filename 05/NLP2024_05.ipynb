{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOC0S1MQP5asxfSfJ3YBriU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karsarobert/NLP2025/blob/main/05/NLP2024_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k"
      ],
      "metadata": {
        "id": "Uy4CZkTkLrVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "593d9efa-0b6d-4548-efd6-84104964c4a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.1)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.1.31)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.17.0)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13539 sha256=963ce35a70036ac3d175dd09d66233531243a8d8729f163479f34a4050448eba\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3342 sha256=0960b3b56ed2d3dbecade5818d61cf976d1782ed5ba1836229bc058df88f6535\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398379 sha256=471fdedc225662c05c48f49f56d8bffbc0007d611a6646983ec815d16446388f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=05dc17207844fdaa7ddad5f863915edec5040eee0042c5e57ab3791a66241de4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml_html_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI9nYStzidYf",
        "outputId": "8e9366c6-2b9f-4b89-8655-c81c4f547695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.1)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.origo.hu/hirarchivum/2024/01/02"
      ],
      "metadata": {
        "id": "BfztUuz_yq20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importáljuk a datetime modult, amely segít kezelni a dátumokat\n",
        "import datetime\n",
        "\n",
        "# Létrehozunk egy üres listát az url-ek tárolására\n",
        "urls = []\n",
        "\n",
        "# Létrehozunk egy datetime objektumot 2023. január 1-jével\n",
        "date = datetime.date(2024, 1, 1)\n",
        "\n",
        "# Létrehozunk egy másik datetime objektumot 2024. január 1-jével\n",
        "end_date = datetime.date(2024, 1, 3)\n",
        "\n",
        "# Végigmegyünk a dátumokon egy napos lépésekkel, amíg el nem érjük a végdátumot\n",
        "while date < end_date:\n",
        "    # Kinyerjük a dátum évét, hónapját és napját\n",
        "    year = date.year\n",
        "    month = date.month\n",
        "    day = date.day\n",
        "    # Összerakjuk az url-t a dátummal\n",
        "    url = f\"https://www.origo.hu/hirarchivum/{year}/{month}/{day}\"\n",
        "    # Hozzáadjuk az url-t a listához\n",
        "    urls.append(url)\n",
        "    # Növeljük a dátumot egy nappal\n",
        "    date += datetime.timedelta(days=1)\n",
        "\n",
        "# Kiírjuk az url-ek listáját\n",
        "print(urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVZehGgcLfVi",
        "outputId": "6292e940-4a31-496a-d118-23124db2bf6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://www.origo.hu/hirarchivum/2024/1/1', 'https://www.origo.hu/hirarchivum/2024/1/2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Egy lista a magyar politikai weboldalakról, amelyekről híreket akarsz kaparni\n",
        "\n",
        "# Egy üres lista a hírek tárolására\n",
        "news = []\n",
        "neutral_news = []\n",
        "itthon_links = []\n",
        "\n",
        "# Végigmegyünk a weboldalak listáján\n",
        "for website in urls:\n",
        "    # Lekérjük a weboldal tartalmát\n",
        "    response = requests.get(website)\n",
        "    # Létrehozunk egy BeautifulSoup objektumot a HTML elemzéséhez\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    # Megkeressük az összes hírcímet tartalmazó HTML elemet (ez változhat az oldaltól függően)\n",
        "    links = soup.find_all(\"a\", href=True)\n",
        "    # Végigmegyünk a hírcímeken\n",
        "    for link in links:\n",
        "      title = \"Nincs cím\"  # Alapértelmezett cím\n",
        "      title_tag = link.find(class_=\"categorized-article-container-title\")\n",
        "      if title_tag is not None:\n",
        "          title = title_tag.text\n",
        "      href = link[\"href\"]\n",
        "    # Folytasd a cikk letöltését és feldolgozását...\n",
        "\n",
        "    # Kinyerjük a hírcím szövegét\n",
        "      href = link[\"href\"]\n",
        "      if href.startswith(\"/itthon\"):\n",
        "          # Létrehozunk egy Article objektumot a linkkel\n",
        "          article = Article(f'https://origo.hu'+href)\n",
        "          # Letöltjük és elemezzük a cikk HTML-jét\n",
        "          try:\n",
        "            article.download()\n",
        "            article.parse()\n",
        "\n",
        "          # Kinyerjük a cikk szövegét\n",
        "            text = article.text\n",
        "          # Hozzáadjuk a hírt a listához egy szótár formájában\n",
        "            news.append({\"title\": title, \"link\": f'https://origo.hu'+href, \"text\": text})\n",
        "          except:\n",
        "            print(\"Hiba történt egy cikk letöltése vagy elemzése közben\")\n",
        "      elif href.startswith('/auto/') or href.startswith('/tudomany/') or href.startswith('/sport/'):\n",
        "          # Létrehozunk egy Article objektumot a linkkel\n",
        "          article = Article(f'https://origo.hu'+href)\n",
        "          # Letöltjük és elemezzük a cikk HTML-jét\n",
        "          article.download()\n",
        "          article.parse()\n",
        "          # Kinyerjük a cikk szövegét\n",
        "          text = article.text\n",
        "          # Hozzáadjuk a hírt a listához egy szótár formájában\n",
        "          neutral_news.append({\"title\": title, \"link\": f'https://origo.hu'+href, \"text\": text})\n",
        "\n",
        "# Létrehozunk egy pandas dataframe-et a hírek listájából\n",
        "df = pd.DataFrame(news)\n",
        "\n",
        "# Elmentjük a dataframe-et egy CSV fájlba\n",
        "df.to_csv(\"origo_news.csv\", index=False)"
      ],
      "metadata": {
        "id": "cM5RaIsTM13v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e2b5fc-8d43-41ff-c65e-7328d95bec7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hiba történt egy cikk letöltése vagy elemzése közben\n",
            "Hiba történt egy cikk letöltése vagy elemzése közben\n",
            "Hiba történt egy cikk letöltése vagy elemzése közben\n",
            "Hiba történt egy cikk letöltése vagy elemzése közben\n",
            "Hiba történt egy cikk letöltése vagy elemzése közben\n",
            "Hiba történt egy cikk letöltése vagy elemzése közben\n",
            "Hiba történt egy cikk letöltése vagy elemzése közben\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article = Article('https://www.origo.hu/itthon/2024/01/jarnak-vonat-nagymaros-tehervonat-veroce')\n",
        "          # Letöltjük és elemezzük a cikk HTML-jét\n",
        "article.download()\n",
        "article.parse()\n",
        "          # Kinyerjük a cikk szövegét\n",
        "text = article.text\n",
        "\n"
      ],
      "metadata": {
        "id": "5mD85kz90XGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    response = requests.get(urls[0])\n",
        "    # Létrehozunk egy BeautifulSoup objektumot a HTML elemzéséhez\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    # Megkeressük az összes hírcímet tartalmazó HTML elemet (ez változhat az oldaltól függően)\n",
        "    links = soup.find_all(\"a\", href=True)"
      ],
      "metadata": {
        "id": "lHE4P3Cu0q-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://telex.hu/archivum?oldal=1"
      ],
      "metadata": {
        "id": "lcMCAIRDBhgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Létrehozunk egy üres listát az url-ek tárolására\n",
        "urls = []\n",
        "start = 1\n",
        "end = 10 # jelenleg 7539 a legnagyobb lehetséges érték\n",
        "\n",
        "# Végigmegyünk a dátumokon egy napos lépésekkel, amíg el nem érjük a végdátumot\n",
        "while start < end:\n",
        "    # Formázzuk a dátumot úgy, hogy két számjegyű legyen a hónap és a nap\n",
        "    date_str = f\"{start}\"\n",
        "    # Összerakjuk az url-t a dátummal\n",
        "    url = f\"https://telex.hu/archivum?oldal={date_str}\"\n",
        "    # Hozzáadjuk az url-t a listához\n",
        "    urls.append(url)\n",
        "    # Növeljük a dátumot egy nappal\n",
        "    start += 1\n",
        "\n",
        "# Kiírjuk az url-ek listáját\n",
        "print(urls)"
      ],
      "metadata": {
        "id": "aJIVO9gsCzBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbdd6eec-58ea-40c2-f725-5edd9e80aea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://telex.hu/archivum?oldal=1', 'https://telex.hu/archivum?oldal=2', 'https://telex.hu/archivum?oldal=3', 'https://telex.hu/archivum?oldal=4', 'https://telex.hu/archivum?oldal=5', 'https://telex.hu/archivum?oldal=6', 'https://telex.hu/archivum?oldal=7', 'https://telex.hu/archivum?oldal=8', 'https://telex.hu/archivum?oldal=9']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://telex.hu/archivum?oldal=1'\n",
        "news = []\n",
        "\n",
        "for website in urls:\n",
        "  try:\n",
        "      response = requests.get(website)\n",
        "      response.raise_for_status()\n",
        "  except requests.exceptions.RequestException as e:\n",
        "      print(e)\n",
        "      sys.exit(1)\n",
        "\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "  articles = soup.find_all('a', class_=\"list__item__title\", href=True)\n",
        "\n",
        "  df = pd.DataFrame(columns=['Title', 'Link', 'Text'])\n",
        "\n",
        "  for article in articles:\n",
        "      if article['href'].find('/belfold/') == 0:\n",
        "        title = article.find(class_=\"hasHighlight\").text.strip()\n",
        "        link = f'https://telex.hu' + article[\"href\"]\n",
        "        #df = df.append({'title': title, 'link': link}, ignore_index=True)\n",
        "        article = Article(link)\n",
        "        # Letöltjük és elemezzük a cikk HTML-jét\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        # Kinyerjük a cikk szövegét\n",
        "        text = article.text\n",
        "        # Hozzáadjuk a hírt a listához egy szótár formájában\n",
        "        news.append({\"title\": title, \"link\": link, \"text\": text})\n",
        "      elif article['href'].find('/kult/') == 0 or article['href'].find('/tudomany/') == 0 or article['href'].find('/sport/') == 0:\n",
        "        title = article.find(class_=\"hasHighlight\").text.strip()\n",
        "        link = f'https://telex.hu' + article[\"href\"]\n",
        "        #df = df.append({'title': title, 'link': link}, ignore_index=True)\n",
        "        article = Article(link)\n",
        "        # Letöltjük és elemezzük a cikk HTML-jét\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        # Kinyerjük a cikk szövegét\n",
        "        text = article.text\n",
        "        # Hozzáadjuk a hírt a listához egy szótár formájában\n",
        "        neutral_news.append({\"title\": title, \"link\": link, \"text\": text})\n",
        "\n",
        "df = pd.DataFrame(news)\n",
        "dfN = pd.DataFrame(neutral_news)\n",
        "df.to_csv(\"telex_news.csv\", index=False)\n",
        "dfN.to_csv(\"neutral_news.csv\", index=False)"
      ],
      "metadata": {
        "id": "r-rwFmQKsCux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "origo = pd.read_csv('origo_news.csv')\n",
        "origo['target']=origo['target']=0\n",
        "telex = pd.read_csv('telex_news.csv')\n",
        "telex['target'] = telex['target']=1\n",
        "neutral = pd.read_csv('neutral_news.csv')\n",
        "neutral['target'] = neutral['target']=2"
      ],
      "metadata": {
        "id": "1Aa12PyCCO4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allnews = pd.concat([origo, telex, neutral])\n"
      ],
      "metadata": {
        "id": "NqNo5BlE6AwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allnews = allnews[['target','text']]"
      ],
      "metadata": {
        "id": "WWqaDrfWCvTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allnews = allnews.dropna()"
      ],
      "metadata": {
        "id": "nV8xQX29A1jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "allnews.to_csv(\"allnews.csv\", index=False)\n",
        "files.download('allnews.csv')"
      ],
      "metadata": {
        "id": "xtzN9dY6MoAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A modell tanítása"
      ],
      "metadata": {
        "id": "Z93vzLoX0Ue7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1pLpK71QeCmXx3yzL6A8oRx2jl1q0J-XJ"
      ],
      "metadata": {
        "id": "7DBIfn0eXLqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02245aac-30e7-4378-9db8-db883b0f2d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pLpK71QeCmXx3yzL6A8oRx2jl1q0J-XJ\n",
            "To: /content/allnews.csv\n",
            "100% 48.2M/48.2M [00:01<00:00, 47.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "allnews = pd.read_csv('allnews.csv')"
      ],
      "metadata": {
        "id": "J9tkUV3M01uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allnews.shape"
      ],
      "metadata": {
        "id": "qGV8DmhT60Ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a8328c-6ea3-4711-897f-c24cb1c98366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20787, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = allnews['text']\n",
        "y = allnews['target']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "wZy6-kny6UAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vec = TfidfVectorizer(ngram_range=(1,3))\n",
        "\n",
        "x_train_tfidf = vec.fit_transform(X_train)\n",
        "x_test_tfidf = vec.transform(X_test)"
      ],
      "metadata": {
        "id": "zUFE32dRTh9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf_tfidf = MultinomialNB(fit_prior=True)\n",
        "clf_tfidf.fit(x_train_tfidf, y_train)\n",
        "y_test_pred_tfidf = clf_tfidf.predict(x_test_tfidf)"
      ],
      "metadata": {
        "id": "s7quiIMaX5AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(y_test, y_test_pred_tfidf) #\n",
        "print(report)"
      ],
      "metadata": {
        "id": "CHV75hgvYg5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81bdd039-64cb-4f6a-ad9e-3c4dc0c71f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.15      0.26      1257\n",
            "           1       1.00      0.00      0.01       796\n",
            "           2       0.69      0.99      0.81      4184\n",
            "\n",
            "    accuracy                           0.70      6237\n",
            "   macro avg       0.83      0.38      0.36      6237\n",
            "weighted avg       0.75      0.70      0.60      6237\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vec = CountVectorizer(analyzer='char', ngram_range=(1,2)) # analyzer='char', ngram_range=(1,5) próbáljuk ki a karakterszintű tokenizálással is\n",
        "\n",
        "x_train_cv = vec.fit_transform(X_train)\n",
        "x_test_cv = vec.transform(X_test)"
      ],
      "metadata": {
        "id": "NSrtRBZ6MTff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB(fit_prior=True) # Az osztály előzetes valószínűségeinek megtanulása, ha hamis akkor egyenletes előfeltevést használunk.\n",
        "clf.fit(x_train_cv, y_train)\n",
        "y_test_pred = clf.predict(x_test_cv)"
      ],
      "metadata": {
        "id": "-kPOa0XuMiRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(y_test, y_test_pred) #\n",
        "print(report)"
      ],
      "metadata": {
        "id": "SZs6I7EgMm2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d9708a-af91-44fe-c3bc-7c2cde40abc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.85      0.62      1257\n",
            "           1       0.70      0.75      0.73       796\n",
            "           2       0.94      0.72      0.81      4184\n",
            "\n",
            "    accuracy                           0.75      6237\n",
            "   macro avg       0.71      0.77      0.72      6237\n",
            "weighted avg       0.82      0.75      0.76      6237\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test.iloc[1])"
      ],
      "metadata": {
        "id": "kzlRXuXoBpoX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ea002e-7c25-4413-dae1-966b9f5a032d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Szijjártó Péter, Magyarország külügyminisztere szerint Dzsudzsák Balázs minden kétséget kizáróan megérdemelte azt, hogy a magyar labdarúgó-válogatott rekordere, 109-szeres válogatott legyen. A politikus nehezen viseli, hogy kedvenc csapata, a Budapest Honvéd gyengén szerepel az NB I-ben. Szerinte Magyarország és Budapest képes lehet egy nyári olimpia megrendezésére. A külügyminiszter a Sportrádiónak adott hosszú interjút, ahol a kérdező lapunk sportrovatvezetője, Lantos Gábor volt. Most ezt a beszélgetést olvashatják el.\n",
            "\n",
            "1989. július 12-én az Amerikai Egyesült Államok elnöke, George Bush Budapesten azt mondta, sportolni szeretne. Először a teniszt választotta, de a partnere elkésett, ezért a Vasas Pasaréti úti telepén futni kezdett, hatalmas fejtörést okozva ezzel a biztonsági embereknek, az elnököt védőknek. Amikor ön New Yorkban az ENSZ Közgyűlés reggelén futócipőt húz, mekkora gondot okoz az ottani bizonságiaknak?\n",
            "\n",
            "Valószínűleg az égvilágon senkinek nem tűnök ott fel, mert az elmondottakhoz képest jóval kisebb stábbal indulunk útnak. Van amúgy egy kedvenc útvonalam New Yorkban. Ha a főkonzulátus épületétől - itt szoktam megszállni, ha a városban vagyok - elindulunk, kimegyünk a folyópartra, majd alulról megkerüljük Manhattant. Mire visszaérünk a konzulátushoz, az pont 21 km. Egy félmaraton távja. Ezt az utat nagyon szeretem, sokszor lefutom, ha a városban vagyok. A Central Park nekem egy kicsit monoton.\n",
            "\n",
            "Amúgy van edzésterve? Vagy ír önnek valaki ilyet? Hiszen akik követik a közösségi oldalát, látják, hogy minden egyes utazása alkalmával sportol.\n",
            "\n",
            "Futásban régen volt. A koronavírus-járvány előtt folyamatosan töltöttem le magamnak ezeket, meg kértem a szakemberek segítségét. Akkor másfél órán belül futottam le a félmaratont, de a járvány ezt is tönkretette. Jelentősen lelassultam. Ami a konditermi edzéseket illeti, ott a feleségemmel együtt követjük a személyi edzőnk tanácsait. Igyekszem minden nap hajnalban a konditeremben megjelenni, sportolni.\n",
            "\n",
            "Három héttel vagyunk a dohai labdarúgó-világbajnokság döntője után. Külügyminiszterként mennyire tudja figyelni a sportvilág történéseit?\n",
            "\n",
            "Igyekszem, alapvetően focirajongó vagyok. Mivel a két gyermekem utánpótláskorú focista, a közös programok kimerülnek a focizásban, a meccsnézésben, vagy a srácok mérkőzéseinek megtekintésében. Én a magyar futballt szeretem, Honvéd-szurkoló vagyok, de bármelyik magyar csapat meccseit szívesen megnézem.\n",
            "\n",
            "A magyar futball és a magyar sport történéseiben otthon vagyok, a nemzetköziben kevésbé, ott inkább a gyerekek focikártyáit nézegetem, s ebből tudom leszűrni, hogy ki mennyit ér a piacon. A vb-döntőt a gyerekeimmel néztem meg, ezt nem lehetett kihagyni.\n",
            "\n",
            "Honvéd-szurkolóként nem fájdalmas megélni a csapat mostani gyengébb szereplését?\n",
            "\n",
            "Én a lelátó hangját tudom tolmácsolni, szakmai kérdésekben nem vagyok kompetens. Nyilván vérzik az ember szíve, hogy manapság minden egyes idényben a bentmaradásért küzd a csapat és ez a küzdelem minden évben egyre tovább tart. Az elmúlt szezonban az utolsó fordulóban dőlt ez el, ami nem volt kellemes érzés. A legutóbbi siker a 2020-as Magyar Kupa-győzelem volt, de azóta valami nem stimmel, ez kétségtelen. Ha végignézek a csapaton, szerintem ez a keret nem a 11. helyre predesztinálja ezt a csapatot. Sokszor van gyomoridegem a meccsek alatt, hogy most mi van a csapattal. Ugyanakkor a magyar futball szurkolójaként örülök annak, hogy van egy olyan együttes - a Ferencváros -, amely ebben az idényben ott lesz a nemzetközi kupaküzdelmek tavaszi folytatásában. A magyar futballt divat volt szidni, meg azokat az intézkedéseket is, amelyeket a sportág felemeléséért hoztunk. De azért szépen lépegetünk előre. A Fradi négy éve folyamatos csoporkörös csapat a nemzetközi kupákban. Ott egy olyan tudatos építkezés zajlott, amire büszke lehet minden magyar futballszurkoló.\n",
            "\n",
            "Ha az életét a futball tölti ki, mi van a futsallal?\n",
            "\n",
            "Az egy régi nagy szerelem volt. Még a Miniszterelnökségen dolgoztam, amikor az ottani kollégákkal kedvtelésből fociztunk.\n",
            "\n",
            "Mivel nem voltunk annyian, hogy egy 5+1-es kispályás csapatot kiállítsunk, jött a futsal. Később Dunakeszin csináltunk egy futsalcsapatot, amely előbb az NB II-ben, majd az NB I-ben is elindult. A csapat egyik fele a közigazgatásban dolgozó futballszeretőkből állt, a másik pedig igazi futsalosokból. Ez akkor óriási élmény volt nekünk.\n",
            "\n",
            "Kevesebben tudják, hogy a kosárlabdához is van kötődése, mert a győri női csapat megszületésénél is ott volt. Maradt ebből a kapcsolatból valami?\n",
            "\n",
            "Már csak emberi, hivatalos nem. Én komáromi születésű vagyok, de Győrben nevelkedtem, a megyében a Győr-Sopron rivalizálás mindig is megvolt. Sopronban akkor két csapat játszott, a mostani Euroliga-győztes együttes elődje és a Soproni Postás, amely megszűnőben volt. Őket vettük át Győrbe. Nagyon sok energiát tettünk ebbe bele, hiszen Győr amúgy kézilabdafőváros volt, sokszoros BL-győztes együttessel. Mára egy komplexen működő, az NB I-ben is jól szereplő és utánpótlás nevelő bázissal rendelkező klub működik Győrben, amire büszkék lehetünk. 2012-ben az NB I-ben is aranyérmet nyertek, ebben a szezonban is nagyszerűen szerepelnek. De nagyon büszkék lehetünk a Sopron Basketre is, amely a tavalyi szezonban megnyerte az Euroligát, amely óriási fegyvertény.\n",
            "\n",
            "A Külügyminisztériumban alakult egy olyan osztály, amelynek az a feladata, hogy felkutassa a határon túl élő magyarok közül azokat, akik tehetséges sportolók és akiket esetleg el lehetne hozni a magyar válogatottakba játszani. Hogy áll most ez a munka?\n",
            "\n",
            "Mi itt segítséget szeretnénk nyújtani. Történelmi okai vannak annak, hogy az ország és a nemzet határai nem esnek egybe. A nemzetközi sportszervezetek ezt a helyzetet nem igazán tudják kezelni. Lehet ma a világ tőlünk távol lévő pontján egy olyan, magyarul már nem beszélő fiatal, akinek a nagyszülei magyarok voltak.\n",
            "\n",
            "Ez a fiatal könnyebben válik papíron magyar sportolóvá, mint az, aki a határon túl él, magyarul beszél és az egykori történelmi Magyarország területén lakik. Neki ugyanis a szülei vagy a nagyszülei soha nem voltak magyar állampolgárok, miközben mindig is magyarok voltak. Ennek a fiatalnak a magyar válogatottakba történő behívása - a nemzetközi sportszervezetek szabályai miatt - sokkal nehezebben megoldható. De mi nem akarunk erőszakosak lenni, segíteni szeretnénk. Az is kell ehhez, hogy az illető, aki a határon túl él, akarja ezt, meg az is, hogy az adott sportági szakszövetség részéről is meg legyen a fogadókészség. De erőszakkal senkit sem akarunk semmibe belekényszeríteni. Viszont ha van ilyen kölcsönös akarat, akkor mi nagyon szívesen segítünk ebben.\n",
            "\n",
            "Miért szeret sportolókkal, volt sportolókkal együtt dolgozni?\n",
            "\n",
            "Mert fegyelmezettek, pontosak, elhivatottak. Az olimpiai bajnok úszó, Gyurta Dániel már régóta a Külügyminisztérium kötelékében dolgozik. A kosárlabdázó Honti Kata, a labdarúgó Vincze Ottó, a vízilabdázó Gór-Nagy Miklós, vagy a teniszező Taróczy Balázs is ott van a csapatban és erre én nagyon büszke vagyok. Ezek a sportolók már sokat küzdöttek a nemzeti színekért. Pontosan tudják, hogy ebben a küzdelemben sokszor a túlerővel szemben kell harcolni. Kőkemény munka van mögöttük. Ezért a diplomáciai akadémián figyelek arra, hogy minél több sportoló kerülhessen be hozzánk. Figyelem is azokat az idősebb sportolókat, akik erre alkalmasak lehetnek. A múltkor, amikor a soproni kosárlabdázók nálam jártak, mondtam a csapat ikonjának, Fegyverneky Zsófiának, hogyha úgy gondolja, szóljon, mert szívesen venném, ha nálunk dolgozna.\n",
            "\n",
            "Amikor Magyarország pályázik egy nagy nemzetközi sporteseményre, mennyt tudnak ebben segíteni?\n",
            "\n",
            "Szinte minden nagy sporteseményért való lobbizásban, kampányolásban és megrendezésében is van szerepünk. Ilyenkor a nagyköveteink népszerűsítik az adott pályázatot. Aztán, amikor eljön a verseny, akkor is van dolgunk, gondoljon csak a vízumkiadásokra, a beutazási engedélyek megadására, a különféle szabályok meghozatalára. Operatív szervezési feladatok ezek.\n",
            "\n",
            "Az idei atlétikai vb megrendezésével egy sor végére érünk, hiszen emellett két vizes vb-t is rendezett már Budapest, társrendezőként benne voltunk a 2021-es labdarúgó Eb lebonyolításában is. A Forma-1-es Magyar Nagydíjak 1986 óta folyamatosan itt vannak, Mogyoródon. Ezek mögött vannak, kisebb sportesemények. De az atlétikai vb megrendezése után adott a kérdés: lesz-e valaha Budapesten nyári olimpia?\n",
            "\n",
            "Nagyon jó lenne. Egyszer már pályáztunk volna, de azt vissza kellett vonni. Én akkor értettem meg, hogy Budapest a maga fekvésével igazi olimpiára termett helyszín lehetne.\n",
            "\n",
            "Nagyon sajnáltam, hogy politikai okok miatt egyesek úgy érezték, hogy tőkét kovácsolhatnak abból, hogy ezt az élményt, ezt a büszkeséget elveszik hazánktól. Utólag könnyen okos az ember, de lett volna esélyünk a pályázat elbírálásakor. Nagyon bízom abban, hogy ez a sansz nem veszett el örökre.\n",
            "\n",
            "A katari focivébé nyitónapján búcsúzott el Dzsudzsák Balázs, aki a görögök elleni meccsen szerepelt utoljára a magyar válogatottban. Ezzel ő lett a válogatottsági csúcstartó, 109-szer szerepelt a nemzeti csapatban. Mi jutott akkor eszébe, amikor ezt a meccset nézte?\n",
            "\n",
            "Sokan tudják azt, hogy nekem Dzsudzsák Balázs a barátom. Nem tudok elfogulatlan véleményt mondani erről. Balázzsal sok időt töltünk együtt, a gyerekeim példaképe. Természetesen eljutott hozzám, hogy voltak olyanok, akik megkérdőjelezték Dzsudzsák pályára lépésének jogosságát, de ezt a felvetést irreálisnak tartottam. Neki ez a búcsúmeccs járt. Ő egy olyan válogatottnak lett a csúcstartója, ahol korábban Puskás Ferenc, Albert Flórián, Kocsis Sándor, Budai László, Göröcs János vagy Nyilasi Tibor játszott. Nem is értettem, hogy merülhetett fel, hogy neki nincs helye a csapatban, a görögök ellen is látszott, hogy bőven befért a csapatba. Mindezzel természetesen nem akarom megkérdőjelezni Marco Rossi szövetségi kapitány későbbi döntéseit, hiszen az eredmények őt minősítik és jó eredményekből szerencsére van bőven.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = vec.transform(pd.Series(['A kormány május 30-án nyújtja be a jövő évi költségvetés tervezetét az Országgyűlésnek, amely július 7-én fogadhatja el azt - mondta Gulyás Gergely Miniszterelnökséget vezető miniszter csütörtöki sajtótájékoztatóján, Budapesten. A tárcavezető kifejtette: számtalan nehézség közepette kell megtervezni a jövő évi büdzsét, \"hiszen háborús időket élünk\". Olyan költségvetésre van szükség, amely garantálja az ország biztonságát, megvédi a családokat, a nyugdíjakat, a munkahelyeket és a rezsicsökkentést - mondta.']))"
      ],
      "metadata": {
        "id": "32vGEY5IFkWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(a):\n",
        "  if a==0:\n",
        "    print('Origo')\n",
        "  elif a==1:\n",
        "    print('Telex')\n",
        "  elif a==2:\n",
        "    print('Semleges')"
      ],
      "metadata": {
        "id": "J6HFfuwNGKeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = clf.predict(a)\n",
        "decode(pred)"
      ],
      "metadata": {
        "id": "gaWMzK6cBuvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3561f499-6f8f-430f-f173-32e86f0a3add"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Origo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {0: \"Origo\", 1: \"Telex\", 2: \"Semleges\"}"
      ],
      "metadata": {
        "id": "YSdv2gLeE1_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label[pred[0]]"
      ],
      "metadata": {
        "id": "PlBSqIkHE4-b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2e0f0d01-a995-4f8c-d7c9-c9877039d823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Origo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oZPSI6d4E8Pj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}